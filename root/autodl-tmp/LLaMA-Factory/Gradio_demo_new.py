import os
import re
import json
import gradio as gr
import requests
from typing import List, Tuple, Dict
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MedicalRAGSystem:
    def __init__(self, data_path: str, api_port: int = 8000):
        self.data_path = data_path
        self.api_port = api_port
        self.api_url = f"http://localhost:{api_port}"
        self.embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')
        self.documents = []
        self.embeddings = None
        self.index = None
        self.metadata = []  # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æ–‡ä»¶åå’Œç« èŠ‚ä¿¡æ¯
        
        # Tokené™åˆ¶è®¾ç½®
        self.max_context_length = 2000
        self.max_doc_length = 500
        self.max_tokens = 1000
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self.load_documents()
        self.create_vector_store()

    def truncate_text(self, text: str, max_length: int) -> str:
        """æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®šé•¿åº¦"""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def load_documents(self):
        """åŠ è½½åŒ»å­¦æ•™ææ–‡æ¡£"""
        logger.info("å¼€å§‹åŠ è½½åŒ»å­¦æ–‡æ¡£...")
        txt_files = list(Path(self.data_path).glob("*.txt"))
        
        for file_path in txt_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                chapters = self.split_by_chapters(content, file_path.name)
                self.documents.extend(chapters)
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    def split_by_chapters(self, content: str, filename: str) -> List[Dict]:
        """æŒ‰ç« èŠ‚åˆ†å‰²æ–‡æ¡£å†…å®¹"""
        chapters = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç« èŠ‚æ ‡é¢˜
        chapter_pattern = r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚|Chapter\s*\d+|Section\s*\d+'
        
        # åˆ†å‰²ç« èŠ‚
        parts = re.split(chapter_pattern, content)
        chapter_titles = re.findall(chapter_pattern, content)
        
        # å¤„ç†ç¬¬ä¸€éƒ¨åˆ†ï¼ˆå¯èƒ½æ²¡æœ‰ç« èŠ‚æ ‡é¢˜ï¼‰
        if parts[0].strip():
            chapters.append({
                'content': self.truncate_text(parts[0].strip(), self.max_doc_length),
                'filename': filename,
                'chapter': 'å‰è¨€æˆ–ä»‹ç»',
                'chapter_index': 0
            })
        
        # å¤„ç†å…¶ä»–ç« èŠ‚
        for i, (title, content_part) in enumerate(zip(chapter_titles, parts[1:]), 1):
            if content_part.strip():
                # è¿›ä¸€æ­¥åˆ†å‰²æˆæ®µè½
                paragraphs = [p.strip() for p in content_part.split('\n\n') if p.strip()]
                for j, paragraph in enumerate(paragraphs):
                    if len(paragraph) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                        chapters.append({
                            'content': self.truncate_text(paragraph, self.max_doc_length),
                            'filename': filename,
                            'chapter': title,
                            'chapter_index': i,
                            'paragraph_index': j
                        })
        
        return chapters

    def create_vector_store(self):
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        if not self.documents:
            logger.error("æ²¡æœ‰æ–‡æ¡£å¯ä»¥åˆ›å»ºå‘é‡å­˜å‚¨")
            return
        
        logger.info("å¼€å§‹åˆ›å»ºå‘é‡å­˜å‚¨...")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = [doc['content'] for doc in self.documents]
        self.metadata = [
            {
                'filename': doc['filename'],
                'chapter': doc['chapter'],
                'chapter_index': doc.get('chapter_index', 0),
                'paragraph_index': doc.get('paragraph_index', 0)
            }
            for doc in self.documents
        ]
        
        # ç”Ÿæˆembeddings
        self.embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        
        # æ ‡å‡†åŒ–embeddingsä»¥ä¾¿ä½¿ç”¨å†…ç§¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        faiss.normalize_L2(self.embeddings)
        self.index.add(self.embeddings.astype('float32'))
        
        logger.info("å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ")

    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, Dict, float]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if self.index is None:
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # æœç´¢
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                results.append((
                    self.documents[idx]['content'],
                    self.metadata[idx],
                    float(score)
                ))
        
        return results

    def build_context_with_citations(self, retrieved_docs: List[Tuple[str, Dict, float]]) -> Tuple[str, str]:
        """æ„å»ºä¸Šä¸‹æ–‡å’Œå¼•ç”¨ä¿¡æ¯ï¼Œç¡®ä¿ä¸è¶…è¿‡é•¿åº¦é™åˆ¶"""
        context_parts = []
        citation_parts = []
        current_length = 0
        
        for i, (content, metadata, score) in enumerate(retrieved_docs, 1):
            doc_text = f"èµ„æ–™{i}ï¼ˆæ¥æºï¼š{metadata['filename']} - {metadata['chapter']}ï¼‰ï¼š{content}"
            
            # æ£€æŸ¥æ·»åŠ è¿™ä¸ªæ–‡æ¡£æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
            if current_length + len(doc_text) > self.max_context_length:
                # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œæˆªæ–­æœ€åä¸€ä¸ªæ–‡æ¡£
                remaining_length = self.max_context_length - current_length
                if remaining_length > 100:  # è‡³å°‘ä¿ç•™100ä¸ªå­—ç¬¦
                    truncated_text = f"èµ„æ–™{i}ï¼ˆæ¥æºï¼š{metadata['filename']} - {metadata['chapter']}ï¼‰ï¼š{content[:remaining_length-50]}..."
                    context_parts.append(truncated_text)
                    # æ·»åŠ æˆªæ–­çš„å¼•ç”¨ä¿¡æ¯
                    citation_parts.append(
                        f"**æ£€ç´¢ä¾æ® {i}ï¼š**\n"
                        f"- æ–‡ä»¶ï¼š{metadata['filename']}\n"
                        f"- ç« èŠ‚ï¼š{metadata['chapter']}\n"
                        f"- ç›¸ä¼¼åº¦ï¼š{score:.3f}\n"
                        f"- å†…å®¹ç‰‡æ®µï¼š{content[:100]}...\n"
                    )
                break
            
            context_parts.append(doc_text)
            current_length += len(doc_text)
            
            # æ·»åŠ å®Œæ•´çš„å¼•ç”¨ä¿¡æ¯
            citation_parts.append(
                f"**æ£€ç´¢ä¾æ® {i}ï¼š**\n"
                f"- æ–‡ä»¶ï¼š{metadata['filename']}\n"
                f"- ç« èŠ‚ï¼š{metadata['chapter']}\n"
                f"- ç›¸ä¼¼åº¦ï¼š{score:.3f}\n"
                f"- å†…å®¹ç‰‡æ®µï¼š{content[:100]}...\n"
            )
        
        context = "\n\n".join(context_parts)
        citations = "\n".join(citation_parts)
        
        return context, citations

    def test_api_connection(self):
        """æµ‹è¯•APIè¿æ¥"""
        try:
            test_response = requests.get(f"{self.api_url}/v1/models", timeout=5)
            logger.info(f"APIå¥åº·æ£€æŸ¥çŠ¶æ€ç : {test_response.status_code}")
            return test_response.status_code == 200
        except requests.exceptions.RequestException as e:
            logger.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def generate_mcq_response(self, question: str, options: Dict[str, str], context: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆé€‰æ‹©é¢˜å›ç­”"""
        # æ„å»ºé€‰æ‹©é¢˜çš„prompt
        options_text = "\n".join([f"{key}: {value}" for key, value in options.items()])
        
        prompt = f"""ä½ æ˜¯ä¸€ååŒ»å­¦åšå£«ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æé—®ç»™å‡ºä¸“ä¸šç­”å¤ï¼Œå¹¶ç»™å‡ºç­”å¤ä¾æ®ã€‚

é¢˜ç›®ï¼š{question}

é€‰é¡¹ï¼š
{options_text}

å‚è€ƒèµ„æ–™ï¼š
{context}

è¯·åˆ†æé¢˜ç›®å’Œé€‰é¡¹ï¼ŒåŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œå¹¶è¯¦ç»†è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ã€‚è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

ç­”æ¡ˆï¼š[é€‰æ‹©çš„é€‰é¡¹å­—æ¯]

è§£æï¼š[è¯¦ç»†çš„è§£é‡Šå’Œæ¨ç†è¿‡ç¨‹]

ä¾æ®ï¼š[åŸºäºå‚è€ƒèµ„æ–™çš„å…·ä½“ä¾æ®]
"""
        
        # å…ˆæµ‹è¯•APIè¿æ¥
        if not self.test_api_connection():
            return "âŒ æ— æ³•è¿æ¥åˆ°LLM APIæœåŠ¡ï¼Œè¯·ç¡®ä¿APIæœåŠ¡æ­£åœ¨è¿è¡Œ"
        
        try:
            payload = {
                "model": "test",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„ç­”æ¡ˆ
                "max_tokens": self.max_tokens
            }
            
            logger.info(f"å‘é€APIè¯·æ±‚ï¼Œprompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            response = requests.post(
                f"{self.api_url}/v1/chat/completions",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=60
            )
            
            logger.info(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    choice = result['choices'][0]
                    if 'message' in choice and 'content' in choice['message']:
                        return choice['message']['content']
                    elif 'text' in choice:
                        return choice['text']
                return f"âœ… APIè°ƒç”¨æˆåŠŸä½†å“åº”æ ¼å¼æœªçŸ¥: {str(result)[:200]}..."
            else:
                error_text = response.text[:500] if response.text else "æ— é”™è¯¯ä¿¡æ¯"
                return f"âŒ APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œé”™è¯¯: {error_text}"
        
        except requests.exceptions.Timeout:
            return "âŒ APIè°ƒç”¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        except requests.exceptions.RequestException as e:
            return f"âŒ APIè¯·æ±‚å¤±è´¥: {str(e)}"
        except json.JSONDecodeError as e:
            return f"âŒ å“åº”JSONè§£æå¤±è´¥: {str(e)}"
        except Exception as e:
            return f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"

    def generate_text_response(self, query: str, context: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆæ–‡æœ¬å›ç­”"""
        # æ„å»ºæ›´ç®€æ´çš„prompt
        prompt = f"""åŸºäºä»¥ä¸‹åŒ»å­¦èµ„æ–™å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{query}

è¯·æä¾›ç®€æ´çš„åŒ»å­¦å›ç­”ï¼š"""
        
        # å…ˆæµ‹è¯•APIè¿æ¥
        if not self.test_api_connection():
            return "âŒ æ— æ³•è¿æ¥åˆ°LLM APIæœåŠ¡ï¼Œè¯·ç¡®ä¿APIæœåŠ¡æ­£åœ¨è¿è¡Œ"
        
        try:
            payload = {
                "model": "test",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.9,
                "max_tokens": self.max_tokens
            }
            
            logger.info(f"å‘é€APIè¯·æ±‚ï¼Œprompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            response = requests.post(
                f"{self.api_url}/v1/chat/completions",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=60
            )
            
            logger.info(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    choice = result['choices'][0]
                    if 'message' in choice and 'content' in choice['message']:
                        return choice['message']['content']
                    elif 'text' in choice:
                        return choice['text']
                
                return f"âœ… APIè°ƒç”¨æˆåŠŸä½†å“åº”æ ¼å¼æœªçŸ¥: {str(result)[:200]}..."
            else:
                error_text = response.text[:500] if response.text else "æ— é”™è¯¯ä¿¡æ¯"
                return f"âŒ APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œé”™è¯¯: {error_text}"
                
        except requests.exceptions.Timeout:
            return "âŒ APIè°ƒç”¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        except requests.exceptions.RequestException as e:
            return f"âŒ APIè¯·æ±‚å¤±è´¥: {str(e)}"
        except json.JSONDecodeError as e:
            return f"âŒ å“åº”JSONè§£æå¤±è´¥: {str(e)}"
        except Exception as e:
            return f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"

    def answer_mcq_question(self, question_json: str) -> Tuple[str, str]:
        """å¤„ç†é€‰æ‹©é¢˜æŸ¥è¯¢"""
        try:
            # è§£æJSONè¾“å…¥
            if isinstance(question_json, str):
                question_data = json.loads(question_json)
            else:
                question_data = question_json
            
            question = question_data.get('question', '')
            options = question_data.get('options', {})
            
            if not question.strip():
                return "è¯·è¾“å…¥å®Œæ•´çš„é¢˜ç›®ä¿¡æ¯ã€‚", ""
            
            if not options:
                return "è¯·æä¾›é€‰æ‹©é¢˜çš„é€‰é¡¹ã€‚", ""
            
            # æ„å»ºæŸ¥è¯¢æ–‡æœ¬ï¼ˆåŒ…å«é—®é¢˜å’Œé€‰é¡¹ï¼‰
            query_text = question + " " + " ".join(options.values())
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = self.retrieve_documents(query_text, top_k=5)
            
            if not retrieved_docs:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„åŒ»å­¦èµ„æ–™ã€‚", ""
            
            # æ„å»ºä¸Šä¸‹æ–‡å’Œå¼•ç”¨ä¿¡æ¯
            context, citations = self.build_context_with_citations(retrieved_docs)
            
            # ç”Ÿæˆå›ç­”
            answer = self.generate_mcq_response(question, options, context)
            
            return answer, citations
        
        except json.JSONDecodeError:
            return "è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„JSONæ ¼å¼ã€‚", ""
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
            return f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}", ""

    def answer_text_query(self, query: str) -> Tuple[str, str]:
        """å¤„ç†æ–‡æœ¬æŸ¥è¯¢"""
        if not query.strip():
            return "è¯·è¾“å…¥æ‚¨çš„åŒ»å­¦é—®é¢˜ã€‚", ""
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_documents(query, top_k=3)
        
        if not retrieved_docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„åŒ»å­¦èµ„æ–™ã€‚", ""
        
        # æ„å»ºä¸Šä¸‹æ–‡å’Œå¼•ç”¨ä¿¡æ¯
        context, citations = self.build_context_with_citations(retrieved_docs)
        
        # ç”Ÿæˆå›ç­”
        answer = self.generate_text_response(query, context)
        
        return answer, citations

# å…¨å±€RAGç³»ç»Ÿå®ä¾‹
rag_system = None

def initialize_rag_system():
    """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    global rag_system
    try:
        rag_system = MedicalRAGSystem(
            data_path="/root/autodl-tmp/LLaMA-Factory/2025_05_22_med_data_zh_paragraph",
            api_port=8000
        )
        return "âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼"
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–RAGç³»ç»Ÿå¤±è´¥: {e}")
        return f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}"

def process_text_query(query: str, history: List[List[str]]) -> Tuple[List[List[str]], str]:
    """å¤„ç†æ–‡æœ¬åŒ»å­¦é—®é¢˜"""
    if rag_system is None:
        return history + [[query, "âŒ è¯·å…ˆåˆå§‹åŒ–RAGç³»ç»Ÿ"]], ""
    
    try:
        answer, citations = rag_system.answer_text_query(query)
        new_history = history + [[query, answer]]
        return new_history, citations
    except Exception as e:
        error_msg = f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        return history + [[query, error_msg]], ""

def process_mcq_question(question_input: str) -> Tuple[str, str]:
    """å¤„ç†åŒ»å­¦é€‰æ‹©é¢˜"""
    if rag_system is None:
        return "âŒ è¯·å…ˆåˆå§‹åŒ–RAGç³»ç»Ÿ", ""
    
    try:
        answer, citations = rag_system.answer_mcq_question(question_input)
        return answer, citations
    except Exception as e:
        error_msg = f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        return error_msg, ""

def clear_text_conversation():
    """æ¸…é™¤æ–‡æœ¬å¯¹è¯å†å²"""
    return [], ""

def clear_mcq_fields():
    """æ¸…é™¤é€‰æ‹©é¢˜å­—æ®µ"""
    return "", "", ""

def create_gradio_interface():
    """åˆ›å»ºGradioç”¨æˆ·ç•Œé¢"""
    with gr.Blocks(title="åŒ»å­¦RAGé—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
        gr.HTML("""
            <h1 style='text-align: center;'>ğŸ¥ åŒ»å­¦RAGé—®ç­”ç³»ç»Ÿ</h1>
            <p style='text-align: center;'>åŸºäº33ä¸ªåŒ»å­¦æ•™æçš„ä¸“ä¸šé—®ç­”ç³»ç»Ÿ - æ”¯æŒæ–‡æœ¬é—®ç­”å’Œé€‰æ‹©é¢˜</p>
        """)
        
        # ç³»ç»Ÿåˆå§‹åŒ–åŒºåŸŸ
        with gr.Row():
            with gr.Column(scale=4):
                init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary", size="lg")
            with gr.Column(scale=2):
                status_output = gr.Textbox(
                    label="ğŸ“Š ç³»ç»ŸçŠ¶æ€",
                    value="è¯·ç‚¹å‡»'åˆå§‹åŒ–ç³»ç»Ÿ'å¼€å§‹ä½¿ç”¨",
                    interactive=False,
                    lines=1
                )
        
        # åˆ›å»ºä¸¤ä¸ªæ ‡ç­¾é¡µ
        with gr.Tabs():
            # æ–‡æœ¬é—®ç­”æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ’¬ æ–‡æœ¬åŒ»å­¦é—®ç­”"):
                with gr.Row():
                    with gr.Column(scale=3):
                        text_chatbot = gr.Chatbot(
                            label="åŒ»å­¦é—®ç­”å¯¹è¯",
                            show_label=True,
                            height=500
                        )
                        
                        with gr.Row():
                            text_query_input = gr.Textbox(
                                placeholder="è¯·è¾“å…¥æ‚¨çš„åŒ»å­¦é—®é¢˜...",
                                label="é—®é¢˜è¾“å…¥",
                                scale=4
                            )
                            text_submit_btn = gr.Button("ğŸ” æé—®", variant="primary", scale=1)
                        
                        text_clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯", variant="secondary")
                    
                    with gr.Column(scale=2):
                        text_citations_output = gr.Markdown(
                            label="ğŸ“š æ£€ç´¢ä¾æ®",
                            value="",
                            height=500
                        )
            
            # é€‰æ‹©é¢˜é—®ç­”æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“ åŒ»å­¦é€‰æ‹©é¢˜"):
                with gr.Row():
                    with gr.Column(scale=3):
                        # è¾“å…¥åŒºåŸŸ
                        with gr.Group():
                            gr.Markdown("### é¢˜ç›®è¾“å…¥")
                            mcq_question_input = gr.Textbox(
                                placeholder='è¯·è¾“å…¥JSONæ ¼å¼çš„é€‰æ‹©é¢˜ï¼Œä¾‹å¦‚ï¼š\n{"question": "ç»è°ƒæŸ¥è¯å®å‡ºç°åŒ»é™¢æ„ŸæŸ“æµè¡Œæ—¶ï¼ŒåŒ»é™¢åº”æŠ¥å‘Šå½“åœ°å«ç”Ÿè¡Œæ”¿éƒ¨é—¨çš„æ—¶é—´æ˜¯ï¼ˆã€€ã€€ï¼‰ã€‚", "options": {"A": "2å°æ—¶", "B": "4å°æ—¶å†…", "C": "8å°æ—¶å†…", "D": "12å°æ—¶å†…", "E": "24å°æ—¶å†…"}}',
                                label="é€‰æ‹©é¢˜è¾“å…¥ï¼ˆJSONæ ¼å¼ï¼‰",
                                lines=6,
                                max_lines=10
                            )
                            
                            with gr.Row():
                                mcq_submit_btn = gr.Button("ğŸ” åˆ†æé¢˜ç›®", variant="primary", scale=4)
                                mcq_clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤", variant="secondary", scale=1)
                        
                        # è¾“å‡ºåŒºåŸŸ
                        with gr.Group():
                            gr.Markdown("### ğŸ¤– AIåˆ†æç»“æœ")
                            mcq_answer_output = gr.Textbox(
                                label="é¢˜ç›®åˆ†æå’Œç­”æ¡ˆ",
                                lines=10,
                                max_lines=15,
                                interactive=False
                            )
                    
                    with gr.Column(scale=2):
                        # æ£€ç´¢ä¾æ®
                        with gr.Group():
                            gr.Markdown("### ğŸ“š æ£€ç´¢ä¾æ®")
                            mcq_citations_output = gr.Markdown(
                                value="",
                                label="æ£€ç´¢æ¥æº",
                                height=500
                            )
        
        # ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ### åŠŸèƒ½ä»‹ç»
            æœ¬ç³»ç»ŸåŸºäº33ä¸ªåŒ»å­¦æ•™æï¼Œæä¾›ä¸¤ç§é—®ç­”æ¨¡å¼ï¼š
            
            #### 1. æ–‡æœ¬åŒ»å­¦é—®ç­”
            - è‡ªç„¶è¯­è¨€å¯¹è¯å½¢å¼çš„åŒ»å­¦é—®ç­”
            - ä¿æŒè¿ç»­çš„å¯¹è¯å†å²
            - æ˜¾ç¤ºç­”æ¡ˆçš„æ£€ç´¢ä¾æ®å’Œå†…å®¹ç‰‡æ®µ
            
            #### 2. åŒ»å­¦é€‰æ‹©é¢˜
            - æ”¯æŒJSONæ ¼å¼çš„é€‰æ‹©é¢˜è¾“å…¥
            - æä¾›è¯¦ç»†çš„é¢˜ç›®åˆ†æå’Œæ¨ç†è¿‡ç¨‹
            - æ˜¾ç¤ºç­”æ¡ˆçš„æ£€ç´¢ä¾æ®å’Œå†…å®¹ç‰‡æ®µ
            
            ### ä½¿ç”¨æ­¥éª¤
            1. **åˆå§‹åŒ–ç³»ç»Ÿ**ï¼šç‚¹å‡»"åˆå§‹åŒ–ç³»ç»Ÿ"æŒ‰é’®ï¼Œç­‰å¾…ç³»ç»ŸåŠ è½½å®Œæˆ
            2. **é€‰æ‹©åŠŸèƒ½**ï¼š
               - æ–‡æœ¬é—®ç­”ï¼šåˆ‡æ¢åˆ°"æ–‡æœ¬åŒ»å­¦é—®ç­”"æ ‡ç­¾é¡µï¼Œç›´æ¥è¾“å…¥é—®é¢˜
               - é€‰æ‹©é¢˜ï¼šåˆ‡æ¢åˆ°"åŒ»å­¦é€‰æ‹©é¢˜"æ ‡ç­¾é¡µï¼Œè¾“å…¥JSONæ ¼å¼çš„é¢˜ç›®
            3. **æŸ¥çœ‹ç»“æœ**ï¼šç³»ç»Ÿä¼šæ˜¾ç¤ºç­”æ¡ˆå’Œç›¸åº”çš„æ£€ç´¢ä¾æ®
            
            ### é€‰æ‹©é¢˜è¾“å…¥æ ¼å¼ç¤ºä¾‹
            ```json
            {
                "question": "ç»è°ƒæŸ¥è¯å®å‡ºç°åŒ»é™¢æ„ŸæŸ“æµè¡Œæ—¶ï¼ŒåŒ»é™¢åº”æŠ¥å‘Šå½“åœ°å«ç”Ÿè¡Œæ”¿éƒ¨é—¨çš„æ—¶é—´æ˜¯ï¼ˆã€€ã€€ï¼‰ã€‚",
                "options": {
                    "A": "2å°æ—¶",
                    "B": "4å°æ—¶å†…", 
                    "C": "8å°æ—¶å†…",
                    "D": "12å°æ—¶å†…",
                    "E": "24å°æ—¶å†…"
                }
            }
            ```
            
            ### æ³¨æ„äº‹é¡¹
            - é¦–æ¬¡åˆå§‹åŒ–å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
            - ç¡®ä¿LLM APIæœåŠ¡æ­£åœ¨è¿è¡Œï¼ˆç«¯å£8000ï¼‰
            - é€‰æ‹©é¢˜è¾“å…¥å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
            - ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³åŒ»å­¦èµ„æ–™å¹¶æä¾›ä¸“ä¸šåˆ†æ
            """)
        
        # äº‹ä»¶ç»‘å®š
        # åˆå§‹åŒ–ç³»ç»Ÿ
        init_btn.click(
            fn=initialize_rag_system,
            outputs=status_output
        )
        
        # æ–‡æœ¬é—®ç­”äº‹ä»¶
        text_submit_btn.click(
            fn=process_text_query,
            inputs=[text_query_input, text_chatbot],
            outputs=[text_chatbot, text_citations_output]
        ).then(
            fn=lambda: "",
            outputs=text_query_input
        )
        
        text_query_input.submit(
            fn=process_text_query,
            inputs=[text_query_input, text_chatbot],
            outputs=[text_chatbot, text_citations_output]
        ).then(
            fn=lambda: "",
            outputs=text_query_input
        )
        
        text_clear_btn.click(
            fn=clear_text_conversation,
            outputs=[text_chatbot, text_citations_output]
        )
        
        # é€‰æ‹©é¢˜äº‹ä»¶
        mcq_submit_btn.click(
            fn=process_mcq_question,
            inputs=[mcq_question_input],
            outputs=[mcq_answer_output, mcq_citations_output]
        )
        
        mcq_clear_btn.click(
            fn=clear_mcq_fields,
            outputs=[mcq_question_input, mcq_answer_output, mcq_citations_output]
        )
    
    return interface

if __name__ == "__main__":
    # å¯åŠ¨Gradioåº”ç”¨
    interface = create_gradio_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )